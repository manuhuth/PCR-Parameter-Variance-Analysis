{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.5 Derive the PC in Practice\n",
    "---\n",
    "\n",
    "The common problem in practice is that usually the VCV matrix of the $X_i$'s $\\pmb C_X$ is unknown. Hence, in practice it is necessary to estimate this matrix. From equation *(a.1*) it is known that the means of the $X_i$'s can be estimated unbiased and every matrix $\\pmb X$ can trasformed into a matrix that has columns coming from a random variable with zero mean. From equation *(a.2)* it is known that the VCV matrix of the $X_i$'s can be estimated unbiased by $\\frac{1}{n-1} \\pmb X' \\pmb X$. We can use this estimate to compute the optimization problem in *(3.3)* with the estimated variances and covariances of the $X_i$'s. However, one can also apply the same fashion as in *(a.2)* to directly compute the estimated VCV matrix for the $Z_m$'s by $\\frac{1}{n-1} \\hat{\\pmb Z}' \\hat{\\pmb Z}$. I use hat notation for the Z matrices, since the variance of $Z_m$ in equation *(3.3)* is estimated, $\\phi_m$ is also estimated and therefore a random variable. Therefore $z_m$ is also estimated, by replacing the true value of $\\phi_m$ in equation *(2.2)* with the estimated value. Consistently, I denote the estimated vectors by $\\hat{\\phi}_m$ and $\\hat{z}_m$. \\\n",
    "In the next step the values in $\\hat{\\pmb \\phi}$ are computed recursively. First, the vector $\\hat{\\phi_1}$ is computed in the same fashion as in *(2.3)* but using the estimated variance. Hence, the empirical maximization problem is\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\phi}_1 = \\arg \\max_{||w|| = 1} \\left( \\frac{1}{n-1} \\hat{z}'_1 \\hat{z}_1 \\right)= \\arg \\max_{||w|| = 1} \\left( \\hat{z}'_1 \\hat{z}_1 \\right)= \\arg \\max_{||w|| = 1} \\left( (\\pmb X w)'(\\pmb X w) \\right) = \\arg \\max_{||w|| = 1} \\left( w' \\pmb X' \\pmb X w\\right)\n",
    "\\tag{3.11}\n",
    "\\end{align}\n",
    "\n",
    "From here onward, the solution is the same as for the theoretical case but estimated variables are used. Thus, I have moved a detailed description to the appendix (*A.4*). The estimated phi matrix consists of estimated eigenvectors, which are the eigenvectors of the estimated variance covariance matrix of the $X_i$'s. As in subsection *2.1.2* the estimated $\\hat{\\pmb \\phi}$ matrix is not unique and its class is denoted by $[\\hat{\\phi}]$.\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\pmb \\phi} = \\begin{pmatrix} \\hat{\\phi_1} & \\hat{\\phi_2 }& \\dots & \\hat{\\phi_M} \\end{pmatrix} = \\begin{pmatrix} \\hat{v_1} & \\hat{v_2 }& \\dots & \\hat{v_M} \\end{pmatrix}\n",
    "\\tag{3.12}\n",
    "\\end{align}\n",
    "\n",
    "The estimated principal components are computed as in *(2.2)* replacing the true $\\pmb \\phi$ by the estimated $\\hat{\\pmb \\phi}$, such that $\\hat{\\pmb Z} = \\pmb X \\hat{\\pmb \\phi}$ is a stochastic matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Principal Component Regression (PCR)\n",
    "---\n",
    "\n",
    "In this subsection I will show how the estimator of the principal component regression can be derived using the principal component analysis. Moreover, I show the differences in the estimates using $\\pmb \\phi$ or $\\hat{\\pmb \\phi}$. First assume a linear true relationship of the expected value of $Y$ and $\\gamma$\n",
    "\n",
    "\\begin{align}\n",
    "Y = \\pmb X \\gamma + \\varepsilon,\n",
    "\\tag{3.13}\n",
    "\\end{align}\n",
    "\n",
    "whereby $\\varepsilon \\in \\mathbb{R}^n$ is a vector of uncorrelated error terms with mean zero, that follows in general some specific distribution and $\\gamma \\in \\mathbb{R}^p$. In most cases, and the subsequent application, this will be a normal distribution. Since the distrbution of the errors is not of major concern in this paper, I will assume, that any vector of error terms follows $\\varepsilon \\sim \\mathcal{N}\\left(\\pmb 0, \\sigma_x^2 \\pmb I_{n \\times n} \\right)$.  \n",
    "\n",
    "The idea of PCR is that $Y$ can be estimated in a linear model by using the constructed $M$ principal components. Recall that $1 \\leq M \\leq p$ to ensure the independency of $Z_m$. The advantage compared to an ordinary OLS model is that often a small number of principal components is sufficient to explain the variability in $X$ and its correlation with $Y$ (Joliffe, [1986](https://link.springer.com/chapter/10.1007/978-1-4757-1904-8_8)). Hence, it is assumed the directions in which the $x_i$ show the most variation are the directions that correspond to $Y$ (James et al., 2013). These directions are the respective eigenvectors of the (estimated) variance covariance matrix $\\pmb C_X$. How much variation is captured by the principal component is given, as stated in equation *(2.11)*, by the proportion of the respective eigenvalue. Feasible choices to find a suitbale M are, for example, choosing a cutoff point $c$ such that $\\lambda_M$ is the smallest eigenvalue such that $\\Phi(\\lambda_M) \\geq c$ or using K-fold cross validation with the average prediction error as objective function. However, in this paper I focus on the variance of the principal component regression and the variance of the predicted outcome and therefore always use all principal components. In equation *(3.19)* I show that the respective betas are invariant to the choice of $M$.\n",
    "\n",
    "The natural question that arises is: Why should that be useful? The line of argumentation in the literature, for example in  Friedmann et al. (2001) and James et al. (2013), is basically the following: Imagine a case where the number of regressors $p$ is close to the number of observations $n$ or a case of very high multicollineaity. In such a case the OLS estimate tends to overfit the data or cannot estimate the coeffcients properly and, thus, yields bad predicion results. Principal component regression circumvents this problem by reducing the dimension of the parameters to estimate. This is true if the true VCV $\\pmb C_X$ is known. However, as stated in the previous subsection, in practice the true variance covariance matrix is not known and $\\pmb \\phi$ is estimated by the eigenvectors of $\\hat{\\pmb C_X}$. So instead of $p$ parameters to estimate, there are $M$ parameters to estimate in the regression and $p$ vectors to estimate before the regression. The latter adds additional randomness to the $M$ estimated parameters. Similar as in the previous subsection, I will first assume that the true $[\\phi]$ class is known and subsequent show what is done in practice, when the variance covariance matrix must be estimated and therefore the class must be estimated by $[\\hat{\\phi}]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 PCR in Theory\n",
    "---\n",
    "\n",
    "As mentioned above, in this subsection it is assumed that the true variance covariance matrix $\\pmb C_X$ is known and therefore the matrix of eigenvectors $\\pmb \\phi$ is deterministic after it is chosen from the class $[\\phi]$. The equation to be estimated is therefore\n",
    "\n",
    "\\begin{align}\n",
    "Y = \\pmb Z \\beta_t + \\varepsilon_Z = \\pmb X \\pmb \\phi \\beta_t + \\varepsilon_Z,\n",
    "\\tag{3.14}\n",
    "\\end{align}\n",
    "\n",
    "whereby $\\varepsilon_Z$ is again a vector of independent homoscedastic error terms defined as in the previous subsection and $\\beta_t \\in \\mathbb{R}^{M}$ is the parameter of interest. The subscript $t$ indicates that the estimator is from the theoretical case computed by the true variance covariance matrix. The model in *(3.15)* is a linear model and can therefore be estimated by minimizing the sum of squarred residuals. The well-known solution to this problem is given by\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\beta_t} = \\arg \\min\\limits_{b \\in \\mathbb{R}^M} ||Y- \\pmb Z b||^2_2 = \\left (\\pmb Z' \\pmb Z \\right)^{-1}\\pmb Z' Y = \\left (\\pmb \\phi' \\pmb X' \\pmb X \\pmb \\phi\\right)^{-1}\\pmb \\phi' \\pmb X' Y.\n",
    "\\tag{3.15}\n",
    "\\end{align}\n",
    "\n",
    "Subsequent, I compute the variance of $\\hat{\\beta_t}$. I make use of ths formula in the simulation study to compare if the formula yields a different variance than simulating the empirical distrubution of $\\hat{\\beta_t}$ and computing the variance from it. This sets the foundation to derive that the variance of stochastic $\\pmb \\phi$ can be estimated by its respectve formula *(3.22)*. \n",
    "\n",
    "I make use of the well-known fact that the OLS esimate is unbiased conditional on $Z$, thus the conditional expectation is constant (ii), and that the conditional variance is given by $\\text{Var}(\\hat{\\beta}_t|\\pmb Z) = \\sigma^2_z \\left(\\pmb Z' \\pmb Z \\right)^{-1}$ (iii). Furthermore, I apply the variance decomposition (i) derived in appendix *(a.8)*. If the variance of $\\varepsilon_Z$ must be estimated the variance of $\\text{Var}(\\hat{\\beta}_t)$ is derived by\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Var}(\\hat{\\beta}_t) \\stackrel{\\text{i}}{=} \\text{E} \\left[\\text{Var}(\\hat{\\beta}_t|\\pmb Z) \\right] + \\text{Var} \\left[\\text{E}(\\hat{\\beta}_t|\\pmb Z) \\right] \\stackrel{\\text{ii}}{=} \\text{E} \\left[\\text{Var}(\\hat{\\beta}_t|\\pmb Z) \\right] \\stackrel{\\text{iii}}{=}   \\text{E} \\left[\\hat{\\sigma}^2_z  \\left( \\pmb Z' \\pmb Z \\right)^{-1} \\right] = \\text{E} \\left[\\hat{\\sigma}^2_z \\left( \\pmb \\phi' \\pmb X' \\pmb X \\pmb \\phi \\right)^{-1} \\right]\n",
    "\\tag{3.16}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
