{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 PCR in Practice\n",
    "---\n",
    "\n",
    "As derived in subsection $2.1.6$, it is assumed in this subsection that the true $\\pmb \\Sigma$ is unknown and therefore the matrix of eigenvectors $\\pmb \\phi$ is estimated by $\\hat{\\pmb \\phi}$ prior the regression. I will denote $\\beta^s$ with the index $s$ to indicate that it is computed using the stochastic $\\hat{\\pmb \\phi}$.\n",
    "\n",
    "\\begin{align}\n",
    "Y =  \\hat{\\pmb Z} \\beta^s + \\varepsilon_Z = \\pmb X \\hat{\\pmb \\phi} \\beta_s + \\varepsilon_Z,\n",
    "\\tag{3.18}\n",
    "\\end{align}\n",
    "\n",
    "The estimate is given analogously to equation *(2.16)* by\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\beta}^s = \\arg \\min\\limits_{b \\in \\mathbb{R}^M} ||Y- \\hat{\\pmb Z} b||^2_2 = \\left (\\hat{\\pmb Z'} \\hat{\\pmb Z} \\right)^{-1} \\hat{\\pmb Z'} Y = \\left (\\hat{\\pmb \\phi'} \\pmb X' \\pmb X \\hat{\\pmb \\phi} \\right)^{-1}\\hat{\\pmb \\phi'} \\pmb X' Y = \\begin{pmatrix} \\frac{\\hat{z}'_1 Y}{\\hat{\\lambda}_1} & \\dots & \\frac{\\hat{z}'_M Y}{\\hat{\\lambda}_M} \\end{pmatrix}'.\n",
    "\\tag{3.19}\n",
    "\\end{align}\n",
    "\n",
    "From equation *(3.19)* we can observe that the sign of the m-th component in $\\hat{\\beta}^s$ is flipped, if the sign of $\\hat{\\phi}_m$, and thus the sign of $\\hat{z}'_m$ is flipped. Hence, $\\hat{\\beta}^s$ is variant to the choice of $\\hat{\\pmb \\phi}$. However, this change in $\\pmb Z$ and $\\hat{\\beta}^s$ does not affect $\\hat{Y}$, which is therefore invariant to the choice of $\\hat{\\pmb \\phi}$\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{Y} = \\hat{Z} \\hat{\\beta}^s = \\hat{Z} \\begin{pmatrix} \\frac{\\hat{z}'_1 Y}{\\hat{\\lambda}_1} & \\dots & \\frac{\\hat{z}'_M Y}{\\hat{\\lambda}_M} \\end{pmatrix}' = \\sum_{m = 1}^M \\frac{z_m z'_m Y}{\\hat{\\lambda}_m} \n",
    "\\tag{3.20}\n",
    "\\end{align}\n",
    "\n",
    "Subsequent, I derive that the estimate conditioned on $\\hat{\\pmb Z}$ yields the true parameter. This finding indicates, using the law of iterated expectations, that the unconditional parameter is unbiased. Hence, the conditional variance of $\\hat{\\pmb  Z}$ can be computed as in the  OLS case yielding $\\text{Var}(\\hat{\\beta}^s|\\pmb Z) = \\sigma^2_z \\left(\\hat{\\pmb Z'} \\hat{\\pmb Z} \\right)^{-1}$. The latter three findings are subsequent used to compute the variance of $\\hat{\\pmb  Z}$, making use of the variance decomposition in the same fashion as in equation *(3.16)*.\n",
    "\n",
    "\\begin{align}\n",
    "\\text{E} \\left(  \\hat{\\beta^s} \\mid \\hat{\\pmb Z} \\right) = \\text{E} \\left[ \\left( \\hat{\\pmb Z'} \\hat{\\pmb Z} \\right)^{-1}\\hat{\\pmb Z'} Y  \\mid \\hat{\\pmb Z} \\right] = \\text{E} \\left[ \\left( \\hat{\\pmb Z'} \\hat{\\pmb Z} \\right)^{-1}\\hat{\\pmb Z'} (\\hat{\\pmb Z}\\beta^s + \\varepsilon_Z)  \\mid \\hat{\\pmb Z} \\right] \n",
    "= \\beta_s +  \\text{E} \\left[ \\left( \\hat{\\pmb Z'} \\hat{\\pmb Z} \\right)^{-1}\\hat{\\pmb Z'} \\varepsilon_Z \\mid \\hat{\\pmb Z}  \\right]  = \\beta^s + \\left( \\hat{\\pmb Z'} \\hat{\\pmb Z} \\right)^{-1}\\hat{\\pmb Z'} \\underbrace{\\text{E}  \\left[\\varepsilon_Z \\mid \\hat{\\pmb Z}  \\right]}_{= 0} = \\beta^s\n",
    "\\tag{3.21}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Var}(\\hat{\\beta}^s) \\stackrel{\\text{i}}{=} \\text{E} \\left[\\text{Var}(\\hat{\\beta^s}|\\hat{\\pmb Z}) \\right] + \\text{Var} \\left[\\text{E}(\\hat{\\beta^s}| \\hat{\\pmb Z}) \\right] \\stackrel{\\text{ii}}{=} \\text{E} \\left[\\text{Var}(\\hat{\\beta^s}|\\hat{\\pmb Z}) \\right] \\stackrel{\\text{iii}}{=} \\text{E} \\left[\\hat{\\sigma}^2_z  \\left(\\hat{\\pmb Z'} \\hat{\\pmb Z} \\right)^{-1} \\right] = \\text{E} \\left[\\hat{\\sigma}^2_z \\left(\\hat{\\pmb \\phi'} \\pmb X' \\pmb X \\hat{\\pmb \\phi} \\right)^{-1} \\right] = \\text{E} \\left[ \\hat{\\sigma}^2_z \\begin{pmatrix} \n",
    "    \\hat{\\lambda}_1 & \\dots & 0\\\\\n",
    "    \\vdots & \\vdots & \\vdots \\\\\n",
    "    0 & \\dots & \\hat{\\lambda}_M\n",
    "\\end{pmatrix}^{-1} \\right] = \\text{E} \\left[\\hat{\\sigma}^2_z \\begin{pmatrix} \n",
    "    \\hat{\\lambda}_1^{-1} & \\dots & 0\\\\\n",
    "    \\vdots & \\vdots & \\vdots \\\\\n",
    "    0 & \\dots & \\hat{\\lambda}_M^{-1}\n",
    "\\end{pmatrix} \\right]\n",
    "\\tag{3.22}\n",
    "\\end{align}\n",
    "and therefore neither the variance nor the expected value are variant to the choice of the representant in $[\\hat{\\phi}]$ if the representation in *(3.22)* is chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Estimate the Variance of $\\hat{\\beta}$ in a Simulation Study\n",
    "---\n",
    "\n",
    "If the notation $\\hat{\\beta}$ is used in this section, this indicates that the given statement holds for for $\\hat{\\beta^s}$ and $\\hat{\\beta^t}$. Since $\\text{Var}(\\hat{Y^s})$ and $\\text{Var}(\\hat{Y^t})$ are invariant to the choice of the matrix of eigenvectors, they can be computed using the empirical distribution as presented for beta subsequently.\n",
    "\n",
    "### 3.3.1 Use the Empirical Distribution of $\\hat{\\beta}$ to Estimate the Variance of $\\hat{\\beta}$ in a Simulation Study\n",
    "\n",
    "For this approach I fix a specific sample size $n$ and simulate $I$ different samples yielding $I$ results for $\\hat{\\beta}$ denoted by $\\hat{\\beta^{(i)}} \\quad i = 1, \\dots, I$ respectively. This beta sample serves as an estimate of the empirical distribution of $\\hat{\\beta}$. Denoting the simulation sample mean of $\\hat{\\beta}^{(i)}$ by $\\overline{\\hat{\\beta}}$, an estimate of the variance $\\text{Var}(\\hat{\\beta})$ can therefore be obtained by \n",
    "\n",
    "\\begin{align}\n",
    "  \\widehat{\\text{Var}(\\hat{\\beta})}^E_{n} = \\frac{1}{I - 1} \\sum_{i = 1}^I \\left(\\hat{\\beta}^{(i)} - \\overline{\\hat{\\beta}} \\right)^2\n",
    "  \\tag{3.23}\n",
    "\\end{align}\n",
    "\n",
    "$ \\widehat{\\text{Var}(\\hat{\\beta})}^E_{n}$ denotes the estimate of the variance of $\\hat{\\beta}$ using the empirical distribution (E) and sample sizes of $n$. A problem that arises using this technique for the case of the estimated eigenvectors is that $\\hat{\\beta_{(i)}}$ is dependent on the choice of $\\hat{\\pmb \\phi}$ and therefore the coefficients vary more than they would do if the representation of $[\\hat{\\phi}]$ would be unique and thus the variance of the coefficient should increase. I examine the size of this effect and find that it does nit occur for any parameter that is estimated in the simulation sudy ins ection *4*.\n",
    "This issue is not faced in the case using the true matrix of eigenvectors, since the representant of $[\\phi]$ is chosen prior to the simulation of the samples. However, this issue can be circumvent by making use of formula *(3.22)*, which yields an estimate of the variance of $\\hat{\\beta^s}$ that is only dependent on the eigenvalues corresponding to $[\\hat{\\phi}]$ and thus is invariant to the choice of $\\hat{\\pmb \\phi}$. The exact formulation of this procedure is presented in the next subsection.\n",
    "\n",
    "### 3.3.2 Use Formula *(3.16)*/*(3.21)* to Estimate the Variance of $\\hat{\\beta}$ in a Simulation Study\n",
    "\n",
    "As stated above, the non-uniqueness of the matrix of eigenvectors in the practical case raises a need to estimate the variance of the coefficient differently. For completness I hold the subsection more general and show not only that that the variance of $\\hat{\\beta^s}$ can be estimated in a simulation by using formula *(3.22)* but also that the variance of $\\hat{\\beta^t}$ can be estimated in a simulation by using formula *(3.16)*. Let $I$ again denote the number of iterations, $n$ a fixed sample size, $\\hat{\\lambda}_k^{(i)}$ the k-th eigenvalue at iteration $i$, $X^{(i)}$ the matrix of regressors at iteration $i$ and $(\\hat{\\sigma}^2_z)_{(i)}$ the estimated error variance at iteration $i$. According to the stated formulas the variance of $\\hat{\\beta}$ can be estimated in a simulation study by computing the simulation sample averages\n",
    "\n",
    "\\begin{align}\n",
    "    \\widehat{\\text{Var}(\\hat{\\beta^s})}^F_{n} = \\frac{1}{I} \\sum _{i = 1}^I (\\hat{\\sigma}^2_z)_{(i)} \\begin{pmatrix} \n",
    "    \\frac{1}{\\hat{\\lambda}_1^{(i)}} & \\dots & 0\\\\\n",
    "    \\vdots & \\vdots & \\vdots \\\\\n",
    "    0 & \\dots & \\frac{1}{\\hat{\\lambda}_M^{(i)}}\n",
    "\\end{pmatrix} \n",
    "\\tag{3.24}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\widehat{\\text{Var}(\\hat{\\beta^t})}^F_{n} =  \\frac{1}{I} \\sum _{i = 1}^I  (\\hat{\\sigma}^2_z)_{(i)} \\left( \\pmb \\phi' \\pmb X'^{(i)} \\pmb X^{(i)} \\pmb \\phi \\right)^{-1} \n",
    "\\tag{3.25}\n",
    "\\end{align}\n",
    "\n",
    "In the simulation study in section 4 I show that for $\\hat{\\beta^tt}$ the procedure in *(3.23)* and the one presented in *(3.25)* yield the same results on average but for $\\hat{\\beta^ss}$ the procedure in *(3.23)* yields a higher variance on average than the one presented in *(3.24)*, due to the non-uniqueness of $\\hat{\\pmb \\phi}$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
