{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "- Earnings Introduction\n",
    "    - why important in economics?\n",
    "    - how to predict? (many highly correlated regressors) usually not all of them can be used\n",
    "    - link to highly correlated regressors\n",
    "    \n",
    "- PCA/PCR Introduction\n",
    "    - what is done? uses eigenvectors from VCV to reduce dimesnion, OLS overfitting if number of obs is small and number of parameters is high. however: VCV usually estimated (differences are emphasized in section 2), increase in unconditional variance of parameter. analytically not tracable without making assumptions about the joint distribution of the estimated eigenvectors and the variables. Hence: simulation (-> also increases variance of estimate.) \n",
    "    - I check this by running a simulation\n",
    "    \n",
    "- structure of paper\n",
    "    - PCA\n",
    "    - PCR\n",
    "    - Structural model and parameters\n",
    "    - simulation\n",
    "    - conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. PCR Methodology\n",
    "--- \n",
    "\n",
    "Let $I = {1, \\dots p}$ be an index set, $x_i \\in \\mathbb{M}_{n \\times 1}$ $\\forall i \\in I$ be a random vector, such that its entries are independent and follow the same distribution $X_i$, with finite first and second. Note that it would be sufficient to assume that the expected values and variances are equal across entries in $x_i$. However, for ease of notation I decided to stick to the case of equal distributions. The variance covariance matrix (VCV) of this random variables is denoted by $\\pmb C_X$. The collection of the $x_i$'s vectors is defined by a matrix \n",
    "\n",
    "\\begin{align}\n",
    "\\pmb X = \\begin{pmatrix} x_1 & x_2 & \\dots & x_p \\end{pmatrix} \\in \\mathbb{M}_{n \\times p}.\n",
    "\\end{align}\n",
    "\n",
    "It is assumed that the random vector $x_i$ is already demeaned by the mean of the random variable $X_i$. Even though it might seem on the first glimpse, this assumption is not very restrictive. It is shown in the appendix *(A.1)* that every matrix for which each entries in the respective column vectors have the same distributions, can be transformed to a matrix such that the expected value of the column vectors is zero. For ease of notation it is therefore assumed that $\\text{E}(X_i) = 0$ $\\forall i \\in I$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Principal Component Analysis (PCA)\n",
    "---\n",
    "\n",
    "### 2.1.1 Aim of PCA\n",
    "---\n",
    "\n",
    "The aim of the PCA is to build $M$ new variables $z_1, z_2, \\dots, z_M$ as orthogonal linear combinations of $x_1, x_2, \\dots, x_p$. Note that $M \\leq p$ otherwise the $z_m$'s cannot be orthogonal to each other. Denoting the scalars that are used to build $z_m$ by $\\phi_m$, one can express $z_m$ as \n",
    "\n",
    "\\begin{align}\n",
    "z_m = \\pmb X \\cdot \\phi_m,\n",
    "\\tag{1.1}\n",
    "\\end{align}\n",
    "\n",
    "whereby $z_m$ is the $m$-th principal component.By defining $\\pmb \\phi = \\begin{pmatrix} \\phi_1 & \\phi_2 & \\dots & \\phi_M \\end{pmatrix}$ it is possible to shorten the above notation. This will later be useful to compute the values for all $z_m$ in one equation.\n",
    "\n",
    "\\begin{align}\n",
    "\\pmb Z = \\begin{pmatrix} z_1 & z_2 & \\dots & z_M \\end{pmatrix} = \\begin{pmatrix} \\pmb X \\cdot \\phi_1 & \\pmb X \\cdot \\phi_2 & \\dots & \\pmb X \\cdot \\phi_M \\end{pmatrix} = \\pmb X \\pmb \\phi.\n",
    "\\tag{1.2}\n",
    "\\end{align}\n",
    "\n",
    "First, I follow many textbooks and take $\\pmb \\phi$ as deterministic. However, in practice it is actually a matrix to estimate and therefore adds additional randomness, turning the $\\pmb Z$ matrix stochastic. The empirical case with unknown true distributions and correlations is discussed in 2.1.6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Distribution of the $Z_m$ variables\n",
    "---\n",
    "\n",
    "The exact distributions are dependent on the distributions of the $X_i$ variables, their convolution properties and the coordiantes $\\phi_m$. Since all $n$ random variables in $z_{m}$ come from the same distribution $Z_m$, they are alle the same linear combinations of random vectors $x_i$ with random variables $X_i$ given by\n",
    "\n",
    "\\begin{align}\n",
    " Z_m  = \\begin{pmatrix} X_1 & X_2 & \\dots & X_p \\end{pmatrix} \\phi_m \\Longleftrightarrow \\mathcal{L}\\left( Z_m \\right) = \\mathcal{L} \\left( \\begin{pmatrix} X_1 & X_2 & \\dots & X_p \\end{pmatrix} \\phi_m \\right)\n",
    "    \\tag{1.3}\n",
    "\\end{align}\n",
    "\n",
    "We take a closer look on the first and second moment in the following. The expected value cof $Z_m$, computed in formula *(1.4)*, is found to be zero.\n",
    "\n",
    "\\begin{align}\n",
    "\\text{E}(Z_m) = \\text{E}(\\begin{pmatrix} X_1 & X_2 & \\dots & X_p \\end{pmatrix} \\cdot \\phi_m) = \\text{E} \\left(\\sum_{j=1}^p X_j \\phi_{jm} \\right) = \\sum_{j=1}^p \\underbrace{E(X_j)}_{= 0} \\phi_{jm} = 0.\n",
    "\\tag{1.4}\n",
    "\\end{align}\n",
    "\n",
    "The variance, derived subsequently, heavily depends on the magnitudes of the enrtries in the vector $\\phi_m$\n",
    "\n",
    "\\begin{align}\n",
    "    \\text{Var}(Z_m) = \\text{E} \\left( \\left[ Z_m - \\underbrace{\\text{E}(Z_m)}_{0} \\right]^2 \\right) =  \\text{E} \\left( Z_m^2 \\right) = \\text{E} \\left( \\begin{pmatrix} X_1 & X_2 & \\dots & X_p \\end{pmatrix}  \\phi_m \\cdot \\phi'_m   \\begin{pmatrix} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_p \\end{pmatrix} \\right) = \\text{E} \\left(\\phi'_m  \\begin{pmatrix} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_p \\end{pmatrix} \\begin{pmatrix} X_1 & X_2 & \\dots & X_p \\end{pmatrix}  \\phi_m \\right) = \\phi'_m \\text{E} \\left( \\begin{pmatrix} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_p \\end{pmatrix} \\begin{pmatrix} X_1 & X_2 & \\dots & X_p \\end{pmatrix} \\right)\\phi_m = \\phi'_m \\pmb C_X  \\phi_m\n",
    "    \\tag{1.5}\n",
    "\\end{align}\n",
    "\n",
    "Note that since $\\pmb C_X$ is a positive definite matrix, it is ensured that the variance of $Z_m$ is always positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
