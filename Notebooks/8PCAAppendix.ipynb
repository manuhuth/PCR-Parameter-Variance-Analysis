{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix - PCA/PCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.1 Every matrix in style of $\\pmb X$ can be transformed to a matrix with random variables of mean zero\n",
    "\n",
    "Let $I = {1, \\dots p}$ be an index set, $x_i \\in \\mathbb{M}_{n \\times 1}$ $\\forall i \\in I$ be a random vector, such that its entries are independent and follow the same distribution $X_i$ that has finite first and second moment. The collection of the $x_i$'s vectors is defined by a matrix \n",
    "\n",
    "\\begin{align}\n",
    "\\pmb X = \\begin{pmatrix} x_1 & x_2 & \\dots & x_p \\end{pmatrix} \\in \\mathbb{M}_{n \\times p}.\n",
    "\\end{align}\n",
    "\n",
    "Since all entries $x_{ji}$ for all $j = 1, \\dots, n$ from a random vector $x_i$, are $i.i.d$ with $\\text{E}\\left(x_{ji} \\right) = \\mu_i$, \n",
    "\n",
    "\\begin{align}\n",
    "    \\overline{x}_i = \\underbrace{\\frac{1}{n} \\text{E}\\left(\\sum_{j = 1}^n x_{ji} \\right)}_{\\text{unbiased estimator of }\\mu_i} = \\frac{1}{n} \\sum_{j = 1}^n \\text{E}\\left(x_{ji}\\right) = \\frac{1}{n} \\sum_{j = 1}^n \\mu_i = \\mu_i\n",
    "\\end{align}\n",
    "\n",
    "is an unbiased estimator such that\n",
    "\\begin{align}\n",
    "\\text{E}\\left(x_{ij} - \\overline{x}_i \\right) = \\text{E}\\left(x_{ij} \\right) - \\text{E}\\left(\\overline{x}_i \\right) = \\mu_i -\\mu_i = 0 \\text{ for all } j = 1, \\dots, n \\text{ and } i \\in I. \n",
    "\\tag{a.1}\n",
    "\\end{align} \n",
    "\n",
    "Therefore, every matrix $\\pmb X$ can be transformed into the desired form with zero means, independent of the magnitude of the $\\mu_i$'s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.2 The matrix product $\\frac{1}{n-1}\\pmb X' \\pmb X$ is an unbiased estimator of the variance-covariance (VCV) matrix of the X_i's\n",
    "\n",
    "Since the mean of the random variables $X_i$ is zero, only one matrix multiplication is needed in order to estimate the VCV matrix of the $X_i$'s.\n",
    "\n",
    "\\begin{align}\n",
    "   \\text{E}\\left(\\frac{1}{n-1}\\pmb X' \\pmb X \\right) = \\frac{1}{n-1} \\text{E}\\left(\\begin{pmatrix} x'_1 \\\\ \\vdots \\\\ x'_p \\end{pmatrix} \\begin{pmatrix} x_1 & \\dots & x_p \\end{pmatrix}\\right) = \n",
    "   \\frac{1}{n-1}\\text{E}\\begin{pmatrix} \n",
    "   x'_1 x_1 & \\dots & x'_1 x_p \\\\\n",
    "   x'_2 x_1 & \\dots & x'_2 x_p \\\\\n",
    "   \\vdots & \\vdots & \\vdots \\\\\n",
    "   x'_p x_1 & \\dots & x'_p x_p \\\\  \n",
    "   \\end{pmatrix} =\n",
    "   \\begin{pmatrix} \\text{var}(X_1)  & \\dots & \\text{cov}(X_1, X_p) \\\\\n",
    "        \\text{cov}(X_2, X_1)  & \\dots & \\text{cov}(X_2, X_p) \\\\\n",
    "        \\vdots &  \\vdots & \\vdots \\\\\n",
    "        \\text{cov}(X_p, X_1) & \\dots & \\text{var}(X_p) \n",
    "    \\end{pmatrix}\n",
    "    \\tag{a.2}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.3 Multiplying a square matrix by a positive scalar does not change its eigenvectors and the order of its eigenvalues\n",
    "\n",
    "Let $\\pmb A \\in \\mathbb{M}_{p \\times p}, \\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_p > 0$ its ordered eigenvalues and $v_1, v_2 \\dots v_n$ the eigenvectors corresponding to the repsective $\\lambda_i$. Let furthermore be $a \\in \\mathbb{R}_{++}, a \\pmb A = \\pmb A'$ and $a \\lambda_i = \\lambda'_i$. From the properties of eigenvectors and eigenvalues it follows for all $i \\in I$\n",
    "\\begin{align}\n",
    "    \\pmb A v_i = \\lambda_i v_i \\Longleftrightarrow a \\pmb A v_i = a \\lambda_i  \\Longleftrightarrow \\pmb A' v_i = \\lambda'_i v_i\n",
    "    \\tag{a.3}\n",
    "\\end{align}\n",
    " \n",
    "Since $a >0$ the order of the different $\\lambda'_i$ is still $\\lambda'_1 \\geq \\lambda'_2 \\geq \\dots \\geq \\lambda'_p > 0$. A side note for latter purposes is that the quotient of an eigenvalue with the sum of all the eigenvalues is also invariant to the multiplication with a positive scalar\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\lambda'_i}{\\sum_{j = 1}^p \\lambda'_j} = \\frac{a \\lambda_i}{\\sum_{j = 1}^p a \\lambda_j} = \\frac{a \\lambda_i}{a \\sum_{j = 1}^p \\lambda_j} = \\frac{\\lambda_i}{\\sum_{j = 1}^p \\lambda_j}\n",
    "    \\tag{a.4}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.4 Derivation of the empirical PC\n",
    "\n",
    "In the following the sam eissue is faced that not a unique matrix of eigenvectors but a whole class of it, is the solution of the problem. Since it follows the same fashion as for the theoretical case, I will not mark this explicitly here again.\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\phi}_1 = \\arg \\max_{||w|| = 1} \\left( \\frac{1}{n-1} \\hat{z}'_1 \\hat{z}_1 \\right)= \\arg \\max_{||w|| = 1} \\left( \\hat{z}'_1 \\hat{z}_1 \\right)= \\arg \\max_{||w|| = 1} \\left( (\\pmb X w)'(\\pmb X w) \\right) = \\arg \\max_{||w|| = 1} \\left( w' \\pmb X' \\pmb X w\\right)\n",
    "\\tag{a.5}\n",
    "\\end{align}\n",
    "\n",
    "with the corresponding lagrangian\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathscr{L}(w,\\lambda) =& w' \\pmb X' \\pmb X w - \\lambda \\left( w'w-1 \\right) \\notag \\\\\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial \\lambda} =& w'w -1 = 0 \\notag \\\\\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial w} =& 2 \\pmb X' \\pmb X w- 2\\lambda w = 0\n",
    "    \\tag{a.6}\n",
    "\\end{align}\n",
    "\n",
    "From equation *(a.6)* it follows that $\\left(\\pmb X' \\pmb X\\right) w = \\lambda w$. Hence, the solution vector w is an eigenvector of $\\pmb X' \\pmb X$ with eigenvalue $\\lambda$. Since $\\pmb X' X$ is a $p \\times p$ matrix, it has $p$ eigenvalues $\\hat{\\lambda_1} \\geq \\hat{\\lambda_2} \\geq \\dots \\geq \\hat{\\lambda_p} > 0$ with p corresponding unit-length eigenvectors $\\hat{v_1}, \\hat{v_2}, \\dots, \\hat{v_p}$. All $\\hat{\\lambda_i}$ are greather than zero, since $X' X$ is positive definite. Thus, the question arises which eigenvalue to use to maximize the variance. Since we can express any eigenvector as a function of the corresponding eigenvalue, it is sufficient to maximize over the eigenvalue.\n",
    "\n",
    "\\begin{align}\n",
    "    \\arg \\max_{\\lambda \\in \\{\\hat{\\lambda_1}, \\dots, \\hat{\\lambda_p} \\}} w' \\pmb X' \\pmb X w \\stackrel{(a.6)}{=}  \\arg \\max_{\\lambda \\in \\{\\hat{\\lambda_1}, \\dots, \\hat{\\lambda_p} \\}} w' \\lambda w = \\arg \\max_{\\lambda \\in \\{\\hat{\\lambda_1}, \\dots, \\hat{\\lambda_p} \\}} \\underbrace{w' w}_{1} \\lambda = \\arg \\max_{\\lambda \\in \\{\\hat{\\lambda_1}, \\dots, \\hat{\\lambda_p} \\}} \\lambda\n",
    "    \\tag{a.7}\n",
    "\\end{align}\n",
    "\n",
    "Hence, the maximum variance is captured by choosing the largest eigenvalue, which is by definition, $\\hat{\\lambda_1}$. The corresponding eigenvector $\\hat{v_1}$ is then chosen to be $\\hat{\\phi_1}$. From equation *(a.3)* it follows that $\\frac{1}{n-1} \\pmb X' \\pmb X$ and $\\pmb X' \\pmb X$ have the same eigenvectors but eigenvalues with the same order multiplied by the scalar. Since it is enough to compute the largest eigenvalue of the estimated VCV matrix of all the $X_i$ to compute $\\hat{\\phi}_1$ and with that $\\hat{z_1}$. Armed with $\\hat{\\phi}_1$, it is possible to compute $\\hat{\\phi}_2, \\hat{\\phi}_3, \\dots, \\hat{\\phi}_M$ iteratively by setting $\\pmb X_m = \\pmb X - \\sum_{j = 1}^{m-1} X \\hat{\\phi}_j \\hat{\\phi'}_j$ and solving the former problem for $\\pmb X_m$ to get $\\hat{\\phi}_m$.\n",
    "\n",
    "It turns out that $\\hat{\\pmb \\phi}$ equals the matrix $\\hat{\\pmb V} = \\begin{pmatrix} \\hat{v_1} & \\hat{v_2 }& \\dots & \\hat{v_M} \\end{pmatrix} $, whereby $\\hat{v_i}$ is the eigenvector with length one of $X'X$ to the corresponding $i$-th highest eigenvalue $\\hat{\\lambda_i}$. Hence, we can compute $\\hat{\\pmb \\phi}$ by only computing the eigenvectors of $X'X$ and sort them in decending order by the corresponding eigenvalues. Subsequent, $\\hat{\\pmb Z}$ is obtained by matrix multiplication of $\\pmb X$ and $\\hat{\\pmb \\phi}$ as in formula *(1.2)*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.5 Law of total variance\n",
    "\n",
    "Let $y \\in \\mathbb{M}_{p \\times 1}$ be a random vector and $x$ be a random matrix. The variance of $y$ can be decomposed by\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Var}(y) =& \\text{E}(yy') - \\text{E}(y)\\text{E}(y') \\\\\n",
    "=& \\text{E} \\left(\\text{E}(yy'|x) \\right) - \\text{E} \\left[ \\text{E}(y|x) \\right] \\text{E} \\left[ \\text{E}(y'|x) \\right] \\\\\n",
    "=& \\text{E} \\left(\\text{Var}(y|x) + \\text{E}(y|x) \\text{E}(y'|x) \\right) - \\text{E} \\left( \\text{E}(y|x) \\right) \\text{E} \\left( \\text{E}(y'|x) \\right) \\\\\n",
    "=& \\text{E} \\left[\\text{Var}(y|x) \\right] + \\text{E} \\left[ \\text{E}(y|x) \\text{E}(y'|x) \\right] - \\text{E} \\left[ \\text{E}(y|x) \\right] \\text{E} \\left[ \\text{E}(y'|x) \\right]\\\\\n",
    "=& \\text{E} \\left[\\text{Var}(y|x) \\right] + \\text{Var} \\left[\\text{E}(y|x) \\right],\n",
    "\\tag{a.8}\n",
    "\\end{align}\n",
    "\n",
    "whereby it is made us of the law of iterated expectations in the second row. The other reshapings are basically applications of the same law given in the first line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
