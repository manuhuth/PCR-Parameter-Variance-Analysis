{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "Project for the course in Computational Statistics | Summer 2020, M.Sc. Economics, Bonn University | [Manuel Huth](https://github.com/manuhuth)\n",
    "\n",
    "# Variance Increase in PCR <a class=\"tocSkip\">   \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook contains ... (one setence)\n",
    "\n",
    "#### Downloading and viewing this notebook\n",
    "\n",
    "* The ensure that every image or format is displayed properly, I recommend to download this notebook from its repository on [GitHub](https://github.com/manuhuth/PCR-Parameter-Variance-Analysis). Other viewing options like _MyBinder_ or _NBViewer_ might have issues to display formulas and formatting.\n",
    "\n",
    "\n",
    "#### Information about the Set up\n",
    "* 2-3 bullet points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read R-Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Introduction\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. PCR Methodology\n",
    "--- \n",
    "\n",
    "## 2.1 General Definitions\n",
    "Let $I = {1, \\dots p}$ be an index set, $x_i \\in \\mathbb{M}_{1 \\times n}$ $\\forall i \\in I$ be a random vector, such that its entries are independent and follow the same distribution $X_i$, and the collection of these vectors is defined by \n",
    "\n",
    "\\begin{align}\n",
    "\\pmb X = \\begin{pmatrix} x_1 & x_2 & \\dots & x_p \\end{pmatrix} \\in \\mathbb{M}_{n \\times p}.\n",
    "\\end{align}\n",
    "\n",
    "It is assumed that the random vector $x_i$ is already demeaned by the mean of the random variable $X_i$. Even though it might seem on the first glimpse, this assumption is not very restrictive.\n",
    "\n",
    "**Every such matrix $\\pmb X$ can be transformed to a matrix with random variables of mean zero**\n",
    "\n",
    "Since all entries $x_{ji}$ for all $j = 1, \\dots, n$ from a random vector $x_i$, are $i.i.d$ with $\\text{E}\\left(x_{ji} \\right) = \\mu_i$, \n",
    "\n",
    "\\begin{align}\n",
    "    \\overline{x}_i = \\underbrace{\\frac{1}{n} \\text{E}\\left(\\sum_{j = 1}^n x_{ji} \\right)}_{\\text{unbiased estimator}} = \\frac{1}{n} \\sum_{j = 1}^n \\text{E}\\left(x_{ji}\\right) = \\frac{1}{n} \\sum_{j = 1}^n \\mu_i = \\mu_i\n",
    "\\end{align}\n",
    "\n",
    "is an unbiased estimator such that\n",
    "\\begin{align}\n",
    "\\text{E}\\left(x_{ij} - \\overline{x}_i \\right) = \\text{E}\\left(x_{ij} \\right) - \\text{E}\\left(\\overline{x}_i \\right) = \\mu_i -\\mu_i = 0 \\text{ for all } j = 1, \\dots, n \\text{ and } i \\in I. \n",
    "\\tag{1.1}\n",
    "\\end{align} \n",
    "\n",
    "Therefore, every matrix $\\pmb X$ can be transformed into the desired form with zero means, independent of the magnitude of the $\\mu_i$'s. For ease of notation it is therefore assumed that $\\text{E}(X_i) = 0$ $\\forall i \\in I$. \n",
    "\n",
    "\n",
    "**The matrix product $\\frac{1}{n-1}\\pmb X' \\pmb X$ is an unbiased estimator of the variance-covariance (VCV) matrix of the X_i's**\n",
    "\n",
    "Since the mean of the random variables $X_i$ is zero, only one matrix multiplication is needed in order to estimate the VCV matrix of the $X_i$'s.\n",
    "\n",
    "\\begin{align}\n",
    "   \\text{E}\\left(\\frac{1}{n-1}\\pmb X' \\pmb X \\right) = \\frac{1}{n-1} \\text{E}\\left(\\begin{pmatrix} x'_1 \\\\ \\vdots \\\\ x'_p \\end{pmatrix} \\begin{pmatrix} x_1 & \\dots & x_p \\end{pmatrix}\\right) = \n",
    "   \\frac{1}{n-1}\\text{E}\\begin{pmatrix} \n",
    "   x'_1 x_1 & \\dots & x'_1 x_p \\\\\n",
    "   x'_2 x_1 & \\dots & x'_2 x_p \\\\\n",
    "   \\vdots & \\vdots & \\vdots \\\\\n",
    "   x'_p x_1 & \\dots & x'_p x_p \\\\  \n",
    "   \\end{pmatrix} =\n",
    "   \\begin{pmatrix} \\text{var}(X_1)  & \\dots & \\text{cov}(X_1, X_p) \\\\\n",
    "        \\text{cov}(X_2, X_1)  & \\dots & \\text{cov}(X_2, X_p) \\\\\n",
    "        \\vdots &  \\vdots & \\vdots \\\\\n",
    "        \\text{cov}(X_p, X_1) & \\dots & \\text{var}(X_p) \n",
    "    \\end{pmatrix}\n",
    "    \\tag{1.2}\n",
    "\\end{align}\n",
    "\n",
    "**Multiplying a square matrix by a positive scalar does not change its eigenvectors and the order of its eigenvalues**\n",
    "\n",
    "Let $\\pmb A \\in \\mathbb{M}_{p \\times p}, \\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_p > 0$ its ordered eigenvalues and $v_1, v_2 \\dots v_n$ the eigenvectors corresponding to the repsective $\\lambda_i$. Let furthermore be $a \\in \\mathbb{R}_{++}, a \\pmb A = \\pmb A'$ and $a \\lambda_i = \\lambda'_i$. From the properties of eigenvectors and eigenvalues it follows for all $i \\in I$\n",
    "\\begin{align}\n",
    "    \\pmb A v_i = \\lambda_i v_i \\Longleftrightarrow \\pmb aA v_i = a \\lambda_i  \\Longleftrightarrow \\pmb A' v_i = \\lambda'_i v_i\n",
    "    \\tag{1.3}\n",
    "\\end{align}\n",
    " \n",
    "Since $a >0$ the order of the different $\\lambda'_i$ is still $\\lambda'_1 \\geq \\lambda'_2 \\geq \\dots \\geq \\lambda'_p > 0$. A side note for latter purposes is that the quotient of an eigenvalue with the sum of all the eigenvalues is also invariant to the multiplication with a positive scalar\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\lambda'_i}{\\sum_{j = 1}^p \\lambda'_j} = \\frac{a \\lambda_i}{\\sum_{j = 1}^p a \\lambda_j} = \\frac{a \\lambda_i}{a \\sum_{j = 1}^p \\lambda_j} = \\frac{\\lambda_i}{\\sum_{j = 1}^p \\lambda_j}\n",
    "    \\tag{1.4}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n",
    "## 2.2 Principal Component Analysis (PCA)\n",
    "\n",
    "**Aim of PCA**\n",
    "\n",
    "The aim of the PCA is to build $M$ new variables $z_1, z_2, \\dots, z_M$ as orthogonal linear combinations of $x_1, x_2, \\dots, x_p$. Note that $M \\leq p$ otherwise the $z_m$'s cannot be orthogonal to each other. Denoting the scalars that are used to build $z_m$ by $\\phi_m$, one can express $z_m$ as \n",
    "\n",
    "\\begin{align}\n",
    "z_m = \\pmb X \\cdot \\phi_m,\n",
    "\\end{align}\n",
    "\n",
    "whereby $z_m$ is the $m$-th principal component.By defining $\\pmb \\phi = \\begin{pmatrix} \\phi_1 & \\phi_2 & \\dots & \\phi_M \\end{pmatrix}$ it is possible to shorten the above notation. This will later be useful to compute the values for all $Z_m$ in one equation.\n",
    "\n",
    "\\begin{align}\n",
    "\\pmb Z = \\begin{pmatrix} Z_1 & Z_2 & \\dots & Z_M \\end{pmatrix} = \\begin{pmatrix} \\pmb X \\cdot \\phi_1 & \\pmb X \\cdot \\phi_2 & \\dots & \\pmb X \\cdot \\phi_M \\end{pmatrix} = \\pmb X \\phi.\n",
    "\\tag{1.5}\n",
    "\\end{align}\n",
    "\n",
    "We follow many textbooks and take $\\pmb \\phi$ as deterministic. However, in practice it is actually a matrix to estimate and therefore adds additional randomness, increasing the variance of the estimated $\\pmb Z$ matrix. \n",
    "\n",
    "**Distribution of the $Z_m$ variables**\n",
    "\n",
    "Note that all $n$ random variables in $z_{m}$ come from the same distribution $Z_m$, since they are alle the same linear combinations of random vectors $x_i$ with random variables $X_i$, and the expected value of the vector $z_m$ can be computed as\n",
    "\n",
    "$$\\text{E}(z_m) = \\text{E}(\\pmb X \\cdot \\phi_m) = \\text{E} \\left(\\sum_{j=1}^p x_j \\phi_{jm} \\right) = \\sum_{j=1}^p \\underbrace{E(x_j)}_{= \\pmb 0} \\phi_{jm} = \\pmb 0.$$\n",
    "\n",
    "Making use of the zero mean and applying formula *(1.2)*, the VCV estimated of the $Z_m$'s can be expressed as $\\frac{1}{n-1} Z' Z$.\n",
    "\n",
    "**Derive the PC**\n",
    "\n",
    "In the next step the values in $\\pmb \\phi$ are computed recursively. First, the vector $\\phi_1$ is computed. Since the goal is to build uncorrelated variables that carry the maximum variance of $\\pmb X$, $\\phi_1$ is constructed such that $Z_1$ has the maxmimum possible variance. Since the variance could be arbitrarily high othwerise, one does only consider $\\phi_1$ with $||\\phi_1|| = 1$. Hence, the maximization problem at hand is\n",
    "\n",
    "\\begin{align}\n",
    "\\phi_1 = \\arg \\max_{||w|| = 1} \\left( \\frac{1}{n-1} z'_1 z_1 \\right)= \\arg \\max_{||w|| = 1} \\left( z'_1 z_1 \\right)= \\arg \\max_{||w|| = 1} \\left( (\\pmb X w)'(\\pmb X w) \\right) = \\arg \\max_{||w|| = 1} \\left( w' \\pmb X' \\pmb X w\\right)\n",
    "\\tag{1.6}\n",
    "\\end{align}\n",
    "\n",
    "To ge the solution of equation *(1.6)* the Lagrangien with the constraint $w' w = 1$ is set up and solved\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathcal{L}(w,\\lambda) =& w' \\pmb X' \\pmb X w - \\lambda \\left( w'w-1 \\right) \\notag \\\\\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial \\lambda} =& w'w -1 = 0 \\notag \\\\\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial w} =& 2 \\pmb X' \\pmb X w- 2\\lambda w = 0\n",
    "    \\tag{1.7}\n",
    "\\end{align}\n",
    "\n",
    "From equation *(1.7)* it follows that $\\left(\\pmb X' \\pmb X\\right) w = \\lambda w$. Hence, the solution vector w is an eigenvector of $\\pmb X' \\pmb X$ with eigenvalue $\\lambda$. Since $\\pmb X' X$ is a $p \\times p$ matrix, it has $p$ eigenvalues $\\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_p > 0$ with p corresponding unit eigenvectors $v_1, v_2, \\dots, v_p$. All $\\lambda_i$ are greather than zero, since $X' X$ is positive definite. Thus, the question arises which eigenvalue to use to maximize the variance. Since we can express any eigenvector as a function of the corresponding eigenvalue, it is sufficient to maximize over the eigenvalue.\n",
    "\n",
    "\\begin{align}\n",
    "    \\arg \\max_{\\lambda \\in \\{\\lambda_1, \\dots, \\lambda_p \\}} w' \\pmb X' \\pmb X w \\stackrel{*(1.7)*}{=}  \\arg \\max_{\\lambda \\in \\{\\lambda_1, \\dots, \\lambda_p \\}} w' \\lambda w = \\arg \\max_{\\lambda \\in \\{\\lambda_1, \\dots, \\lambda_p \\}} \\underbrace{w' w}_{1} \\lambda = \\arg \\max_{\\lambda \\in \\{\\lambda_1, \\dots, \\lambda_p \\}} \\lambda\n",
    "    \\tag{1.8}\n",
    "\\end{align}\n",
    "\n",
    "Hence, the maximum variance is captured by choosing the largest eigenvalue, which is by definition, $\\lambda_1$. The corresponding eigenvector is the $v_1$ is then chosen to be $\\phi_1$. From equation *(1.4)* it follows that $\\frac{1}{n-1} \\pmb X' \\pmb X$ and $\\pmb X' \\pmb X$ have the same eigenvectors but eigenvalues with the same order multiplied by the scalar. Since it is enough to compute the largest eigenvalue of the VCV matrix of all the $X_i$ to compute $\\phi_1$ and with that $z_1$. Armed with $\\phi_1$, we can now compute $\\phi_2, \\phi_3, \\dots, \\phi_M$ iteratively by setting $\\pmb X_m = \\pmb X - \\sum_{j = 1}^{m-1} X \\phi'_j \\phi_j$ and solving the former problem for $\\pmb X_m$ to get $\\phi_m$. \n",
    "\n",
    "It turns out that $\\pmb \\phi$ equals the matrix $\\pmb V = \\begin{pmatrix} v_1 & v_2 & \\dots & v_M \\end{pmatrix} $, whereby $v_i$ is the eigenvector with length one of $X'X$ to the corresponding $i$-th highest eigenvalue $\\lambda_i$. Hence, we can compute $\\pmb \\phi$ by only computing the eigenvectors of $X'X$ and sort them in decending order by the corresponding eigenvalues. Subsequent, $\\pmb Z$ is obtained by matrix multiplication of $\\pmb X$ and $\\pmb \\phi$ as in formula *(1.5)*.\n",
    "\n",
    "**Computed Principal Components have covariance zero**\n",
    "\n",
    "To show that the PC are indeed uncorrelated, it is sufficient to compute the VCV for it. Note that $z'_i z_j = 0$, if $i \\neq j$ since $z_i$ and $z_j$ are from different eigenspaces and therefore they are orthogonal. The above shown VCV formula of the random $Z_m$ variables with mean zero is extended by \n",
    "\n",
    "\\begin{align}\n",
    "\\frac{1}{n-1} \\pmb Z' \\pmb Z =& \\pmb \\phi' \\pmb X' \\pmb X \\pmb \\phi =\n",
    "\\begin{pmatrix} \n",
    "    v'_1 \\\\ \\vdots \\\\ v'_M \n",
    "\\end{pmatrix}\n",
    " \\pmb X' \\pmb X \n",
    " \\begin{pmatrix} \n",
    "    v_1 & \\dots & v_M \n",
    "\\end{pmatrix}\n",
    "\\stackrel{\\text{eigendecomposition}}{=} \n",
    "\\begin{pmatrix} \n",
    "    v'_1 \\\\ \\vdots \\\\ v'_M \n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix} \n",
    "    v_1 & \\dots & v_M \n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix} \n",
    "    \\lambda_1 & \\dots & 0\\\\\n",
    "    \\vdots & \\vdots & \\vdots \\\\\n",
    "    0 & \\dots & \\lambda_M\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix} \n",
    "    v'_1 \\\\ \\vdots \\\\ v'_M \n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix} \n",
    "    v_1 & \\dots & v_M \n",
    "\\end{pmatrix}\n",
    "\\\\\n",
    "=& \n",
    "\\begin{pmatrix} \n",
    "    v'_1 v_1 & \\dots & 0 \\\\ \\vdots & \\vdots & \\vdots \\\\ 0 & \\dots & v'_M v_M \n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix} \n",
    "    \\lambda_1 & \\dots & 0\\\\\n",
    "    \\vdots & \\vdots & \\vdots \\\\\n",
    "    0 & \\dots & \\lambda_M\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix} \n",
    "    v'_1 v_1 & \\dots & 0 \\\\ \\vdots & \\vdots & \\vdots \\\\ 0 & \\dots & v'_M v_M \n",
    "\\end{pmatrix}\n",
    "= \n",
    "\\begin{pmatrix} \n",
    "    1 & \\dots & 0 \\\\ \\vdots & \\vdots & \\vdots \\\\ 0 & \\dots & 1 \n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix} \n",
    "    \\lambda_1 & \\dots & 0\\\\\n",
    "    \\vdots & \\vdots & \\vdots \\\\\n",
    "    0 & \\dots & \\lambda_M\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix} \n",
    "    1 & \\dots & 0 \\\\ \\vdots & \\vdots & \\vdots \\\\ 0 & \\dots & 1\n",
    "\\end{pmatrix}\n",
    "= \\begin{pmatrix} \n",
    "    \\lambda_1 & \\dots & 0\\\\\n",
    "    \\vdots & \\vdots & \\vdots \\\\\n",
    "    0 & \\dots & \\lambda_M\n",
    "\\end{pmatrix}\n",
    "\\tag{1.9}\n",
    "\\end{align} \n",
    "\n",
    "As already shown, the variance of $Z_m$ is the $m$-th largest eigenvalue $\\lambda_m$. The non-diagonal elements of the VCV are all zero, which indicates that the vcovariances of the $Z_m$'s are indeed zero.  \n",
    "**Portion of cariance captured by the $\\lambda_i$'s**\n",
    "\n",
    "What we also learn from the derivation of equation *(1.8)* is that $\\text{Var}(Z_i) = \\lambda_i \\forall i \\in I$, the $i$-th highest eigenvalue equals the variance of the $i$-th PC. Hence  \n",
    "\n",
    "\n",
    "\n",
    "## 2.3 Principal Component Regression (PCR)\n",
    "be the matrix of indepedent variables and $\\varepsilon \\in \\mathbb{M}_{1 \\times n}$ a random vector.\n",
    "The data generating process is $Y = \\pmb X \\beta + \\varepsilon$. For ease of notation, we assume the expected value of any covariate and of the dependent variable to be $0$. Hence, $\\text{E} ( \\pmb X ) = \\pmb0$ i.e. $\\text{E}(X_i) = 0$ for $\\forall i$. Since we can demean any linear model, this is not restrictive in any sense.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Structural Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivaton of pca\n",
    "https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch18.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get number of PCA components\n",
    " Kaiser  criterion(Guttman, 1954; Kaiser, 1960)\n",
    " acceleration  factor(Cattell,  1966; Raiche,  Roipel,  and  Blais,2006)and parallel analysis(Horn, 1965)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comparison of pca and ridge\n",
    "https://www.researchgate.net/publication/259265422_A_Monte_Carlo_Comparison_between_Ridge_and_Principal_Components_Regression_Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction\n",
    "- Black  (1997)  shows  that  parents  arewilling to pay a premium to buy a house in a neighborhood with aschool that scores well. (Do  Better  Schools  Matter?  Parental  Valuation  ofElementary  Education) - Havard paper\n",
    "- https://www.researchgate.net/profile/Duncan_Thomas/publication/5194918_Early_Test_Scores_Socioeconomic_Status_and_Future_Outcomes/links/575812c508ae5c6549074510/Early-Test-Scores-Socioeconomic-Status-and-Future-Outcomes.pdf nice to find literature on test scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. structural model with equation\n",
    "2. relationship $y = X \\beta + \\epsilon$\n",
    "    Xs are correlated -> pcr\n",
    "3. simulate structural model using parameter\n",
    "4. see how large the variance is of the pcr beta and compare to the ones estimated. (can be computed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
