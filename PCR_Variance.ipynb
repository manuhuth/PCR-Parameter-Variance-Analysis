{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "Project for the course in Computational Statistics | Summer 2020, M.Sc. Economics, Bonn University | [Manuel Huth](https://github.com/manuhuth)\n",
    "\n",
    "# Variance Increase in PCR <a class=\"tocSkip\">   \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook contains ... (one setence)\n",
    "\n",
    "#### Downloading and viewing this notebook\n",
    "\n",
    "* The ensure that every image or format is displayed properly, I recommend to download this notebook from its repository on [GitHub](https://github.com/manuhuth/PCR-Parameter-Variance-Analysis). Other viewing options like _MyBinder_ or _NBViewer_ might have issues to display formulas and formatting.\n",
    "\n",
    "\n",
    "#### Information about the Set up\n",
    "* 2-3 bullet points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read R-Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Introduction\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. PCR Methodology\n",
    "--- \n",
    "\n",
    "## 2.1 General definitions\n",
    "Let $I = {1, \\dots p}$ be an index set, $x_i \\in \\mathbb{M}_{n \\times 1}$ $\\forall i \\in I$ be a random vector, such that its entries are independent and follow the same distribution $X_i$ with variance covariance matrix (VCV) of this random variables $\\pmb C_X$. The collection of the $x_i$'s vectors is defined by a matrix \n",
    "\n",
    "\\begin{align}\n",
    "\\pmb X = \\begin{pmatrix} x_1 & x_2 & \\dots & x_p \\end{pmatrix} \\in \\mathbb{M}_{n \\times p}.\n",
    "\\end{align}\n",
    "\n",
    "It is assumed that the random vector $x_i$ is already demeaned by the mean of the random variable $X_i$. Even though it might seem on the first glimpse, this assumption is not very restrictive.\n",
    "\n",
    "**Every such matrix $\\pmb X$ can be transformed to a matrix with random variables of mean zero**\n",
    "\n",
    "Since all entries $x_{ji}$ for all $j = 1, \\dots, n$ from a random vector $x_i$, are $i.i.d$ with $\\text{E}\\left(x_{ji} \\right) = \\mu_i$, \n",
    "\n",
    "\\begin{align}\n",
    "    \\overline{x}_i = \\underbrace{\\frac{1}{n} \\text{E}\\left(\\sum_{j = 1}^n x_{ji} \\right)}_{\\text{unbiased estimator}} = \\frac{1}{n} \\sum_{j = 1}^n \\text{E}\\left(x_{ji}\\right) = \\frac{1}{n} \\sum_{j = 1}^n \\mu_i = \\mu_i\n",
    "\\end{align}\n",
    "\n",
    "is an unbiased estimator such that\n",
    "\\begin{align}\n",
    "\\text{E}\\left(x_{ij} - \\overline{x}_i \\right) = \\text{E}\\left(x_{ij} \\right) - \\text{E}\\left(\\overline{x}_i \\right) = \\mu_i -\\mu_i = 0 \\text{ for all } j = 1, \\dots, n \\text{ and } i \\in I. \n",
    "\\tag{1.1}\n",
    "\\end{align} \n",
    "\n",
    "Therefore, every matrix $\\pmb X$ can be transformed into the desired form with zero means, independent of the magnitude of the $\\mu_i$'s. For ease of notation it is therefore assumed that $\\text{E}(X_i) = 0$ $\\forall i \\in I$. \n",
    "\n",
    "\n",
    "**The matrix product $\\frac{1}{n-1}\\pmb X' \\pmb X$ is an unbiased estimator of the variance-covariance (VCV) matrix of the X_i's**\n",
    "\n",
    "Since the mean of the random variables $X_i$ is zero, only one matrix multiplication is needed in order to estimate the VCV matrix of the $X_i$'s.\n",
    "\n",
    "\\begin{align}\n",
    "   \\text{E}\\left(\\frac{1}{n-1}\\pmb X' \\pmb X \\right) = \\frac{1}{n-1} \\text{E}\\left(\\begin{pmatrix} x'_1 \\\\ \\vdots \\\\ x'_p \\end{pmatrix} \\begin{pmatrix} x_1 & \\dots & x_p \\end{pmatrix}\\right) = \n",
    "   \\frac{1}{n-1}\\text{E}\\begin{pmatrix} \n",
    "   x'_1 x_1 & \\dots & x'_1 x_p \\\\\n",
    "   x'_2 x_1 & \\dots & x'_2 x_p \\\\\n",
    "   \\vdots & \\vdots & \\vdots \\\\\n",
    "   x'_p x_1 & \\dots & x'_p x_p \\\\  \n",
    "   \\end{pmatrix} =\n",
    "   \\begin{pmatrix} \\text{var}(X_1)  & \\dots & \\text{cov}(X_1, X_p) \\\\\n",
    "        \\text{cov}(X_2, X_1)  & \\dots & \\text{cov}(X_2, X_p) \\\\\n",
    "        \\vdots &  \\vdots & \\vdots \\\\\n",
    "        \\text{cov}(X_p, X_1) & \\dots & \\text{var}(X_p) \n",
    "    \\end{pmatrix}\n",
    "    \\tag{1.2}\n",
    "\\end{align}\n",
    "\n",
    "**Multiplying a square matrix by a positive scalar does not change its eigenvectors and the order of its eigenvalues**\n",
    "\n",
    "Let $\\pmb A \\in \\mathbb{M}_{p \\times p}, \\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_p > 0$ its ordered eigenvalues and $v_1, v_2 \\dots v_n$ the eigenvectors corresponding to the repsective $\\lambda_i$. Let furthermore be $a \\in \\mathbb{R}_{++}, a \\pmb A = \\pmb A'$ and $a \\lambda_i = \\lambda'_i$. From the properties of eigenvectors and eigenvalues it follows for all $i \\in I$\n",
    "\\begin{align}\n",
    "    \\pmb A v_i = \\lambda_i v_i \\Longleftrightarrow \\pmb aA v_i = a \\lambda_i  \\Longleftrightarrow \\pmb A' v_i = \\lambda'_i v_i\n",
    "    \\tag{1.3}\n",
    "\\end{align}\n",
    " \n",
    "Since $a >0$ the order of the different $\\lambda'_i$ is still $\\lambda'_1 \\geq \\lambda'_2 \\geq \\dots \\geq \\lambda'_p > 0$. A side note for latter purposes is that the quotient of an eigenvalue with the sum of all the eigenvalues is also invariant to the multiplication with a positive scalar\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\lambda'_i}{\\sum_{j = 1}^p \\lambda'_j} = \\frac{a \\lambda_i}{\\sum_{j = 1}^p a \\lambda_j} = \\frac{a \\lambda_i}{a \\sum_{j = 1}^p \\lambda_j} = \\frac{\\lambda_i}{\\sum_{j = 1}^p \\lambda_j}\n",
    "    \\tag{1.4}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n",
    "## 2.2 Principal Component Analysis (PCA)\n",
    "\n",
    "**Aim of PCA**\n",
    "\n",
    "The aim of the PCA is to build $M$ new variables $z_1, z_2, \\dots, z_M$ as orthogonal linear combinations of $x_1, x_2, \\dots, x_p$. Note that $M \\leq p$ otherwise the $z_m$'s cannot be orthogonal to each other. Denoting the scalars that are used to build $z_m$ by $\\phi_m$, one can express $z_m$ as \n",
    "\n",
    "\\begin{align}\n",
    "z_m = \\pmb X \\cdot \\phi_m,\n",
    "\\end{align}\n",
    "\n",
    "whereby $z_m$ is the $m$-th principal component.By defining $\\pmb \\phi = \\begin{pmatrix} \\phi_1 & \\phi_2 & \\dots & \\phi_M \\end{pmatrix}$ it is possible to shorten the above notation. This will later be useful to compute the values for all $Z_m$ in one equation.\n",
    "\n",
    "\\begin{align}\n",
    "\\pmb Z = \\begin{pmatrix} Z_1 & Z_2 & \\dots & Z_M \\end{pmatrix} = \\begin{pmatrix} \\pmb X \\cdot \\phi_1 & \\pmb X \\cdot \\phi_2 & \\dots & \\pmb X \\cdot \\phi_M \\end{pmatrix} = \\pmb X \\phi.\n",
    "\\tag{1.5}\n",
    "\\end{align}\n",
    "\n",
    "We follow many textbooks and take $\\pmb \\phi$ as deterministic. However, in practice it is actually a matrix to estimate and therefore adds additional randomness, increasing the variance of the estimated $\\pmb Z$ matrix. \n",
    "\n",
    "**Distribution of the $Z_m$ variables**\n",
    "\n",
    "The exact distributions are dependent on the distributions of the $X_i$ variables, their convolution properties and the coordiantes $\\phi_m$. Since all $n$ random variables in $z_{m}$ come from the same distribution $Z_m$, they are alle the same linear combinations of random vectors $x_i$ with random variables $X_i$ given by\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathcal{L}\\left( Z_m \\right) = \\mathcal{L} \\left( \\begin{pmatrix} X_1 & X_2 & \\dots & X_p \\end{pmatrix} \\phi_m \\right)\n",
    "    \\tag{1.6}\n",
    "\\end{align}\n",
    "\n",
    "We take a closer look on the first and second moment in the following. The expected value cof $Z_m$, computed in formula *(1.7)*, is found to be zero.\n",
    "\n",
    "\\begin{align}\n",
    "\\text{E}(Z_m) = \\text{E}(\\begin{pmatrix} X_1 & X_2 & \\dots & X_p \\end{pmatrix} \\cdot \\phi_m) = \\text{E} \\left(\\sum_{j=1}^p X_j \\phi_{jm} \\right) = \\sum_{j=1}^p \\underbrace{E(X_j)}_{= 0} \\phi_{jm} = 0.\n",
    "\\tag{1.7}\n",
    "\\end{align}\n",
    "\n",
    "The variance, derived subsequently, heavily depends on the magnitudes of the enrtries in the vector $\\phi_m$\n",
    "\n",
    "\\begin{align}\n",
    "    \\text{Var}(Z_m) = \\text{E} \\left( \\left[ Z_m - \\underbrace{\\text{E}(Z_m)}_{0} \\right]^2 \\right) =  \\text{E} \\left( Z_m \\right) = \\text{E} \\left( \\begin{pmatrix} X_1 & X_2 & \\dots & X_p \\end{pmatrix}  \\phi_m \\cdot \\phi'_m   \\begin{pmatrix} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_p \\end{pmatrix} \\right) = \\text{E} \\left(\\phi'_m  \\begin{pmatrix} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_p \\end{pmatrix} \\begin{pmatrix} X_1 & X_2 & \\dots & X_p \\end{pmatrix}  \\phi_m \\right) = \\phi'_m \\text{E} \\left( \\begin{pmatrix} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_p \\end{pmatrix} \\begin{pmatrix} X_1 & X_2 & \\dots & X_p \\end{pmatrix} \\right)\\phi_m = \\phi'_m \\pmb C_X  \\phi_m\n",
    "    \\tag{1.8}\n",
    "\\end{align}\n",
    "\n",
    "Note that since $\\pmb C$ is a positive definite matrix, it is ensured that the variance of $Z_m$ is always positive.\n",
    "\n",
    "**Derive the PC theoretically**\n",
    "\n",
    "In the next step it is discussed how the $\\phi_m$ are chosen in order to ensure uncorrelated $Z_m$'s and such that $\\text{Var}(Z_m) \\geq \\text{Var}(Z_{m+1}) \\forall m \\in I_{-p}$. The first coordinates $\\phi_1$ must be chosen in order to maximize the variance of the random variable $Z_1$. Recall from the previous argument *(1.8)* that the distribution of $Z_m$ is a function of the coordinates $\\phi_m$ with expected value zero. Since the variance of $Z_m$ can be arbitrarily high, see formula *(1.8)*, one does restrict the choice of $\\phi_m$ to vectors of length one.\n",
    "\n",
    "\\begin{align}\n",
    "    \\phi_1 = \\arg \\max_{||w|| = 1} \\text{Var}(Z_1) = \\phi'_1 \\pmb C_X  \\phi_1\n",
    "    \\tag{1.9}\n",
    "\\end{align}\n",
    "\n",
    "This problem can be solved using the lagrangian with the constraint $w' w = 1$ \n",
    "\\begin{align}\n",
    "    \\mathscr{L}(w,\\lambda) =& w' \\pmb C_X w - \\lambda \\left( w'w-1 \\right) \\notag \\\\\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial \\lambda} =& w'w -1 = 0 \\notag \\\\\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial w} =& 2 \\pmb C_X w - 2\\lambda w = 0\n",
    "    \\tag{1.10}\n",
    "\\end{align}\n",
    "\n",
    "From equation *(1.10)* it follows that $\\left(\\pmb C_X \\right) w = \\lambda w$. Hence, the solution vector w is an eigenvector of $\\pmb C_X$ with eigenvalue $\\lambda$. Since $\\pmb C_X$ is a $p \\times p$ matrix, it has $p$ eigenvalues $\\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_p > 0$ with p corresponding unit-length eigenvectors $v_1, v_2, \\dots, v_p$. All $\\lambda_i$ are greather than zero, since $\\pmb C_X$ is positive definite. Thus, the question arises which eigenvalue to use to maximize the variance. Since we can express any eigenvector as a function of the corresponding eigenvalue, it is sufficient to maximize over the eigenvalue.\n",
    "\n",
    "\\begin{align}\n",
    "    \\arg \\max_{\\lambda \\in \\{\\lambda_1, \\dots, \\lambda_p \\}} w' \\pmb C_X w \\stackrel{*(1.21)*}{=}  \\arg \\max_{\\lambda \\in \\{\\lambda_1, \\dots, \\lambda_p \\}} w' \\lambda w = \\arg \\max_{\\lambda \\in \\{\\lambda_1, \\dots, \\lambda_p \\}} \\underbrace{w' w}_{1} \\lambda = \\arg \\max_{\\lambda \\in \\{\\lambda_1, \\dots, \\lambda_p \\}} \\lambda\n",
    "    \\tag{1.11}\n",
    "\\end{align}\n",
    "\n",
    "Hence, the maximum variance is captured by choosing the largest eigenvalue, which is by definition, $\\lambda_1$. The corresponding eigenvector $v_1$ is then chosen to be $\\phi_1$. Thus, it is enough to compute the largest eigenvalue of the VCV matrix of all the $X_i$ to compute $\\phi_1$ and with that $z_1$. Armed with $\\phi_1$, it is possible to compute $\\phi_2, \\phi_3, \\dots, \\phi_M$ iteratively by defining new random variables \n",
    "\n",
    "\\begin{align}\n",
    "\\begin{pmatrix} X_1^{(m)} \\\\ X_2^{(m)} \\\\ \\vdots \\\\ X_p^{(m)} \\end{pmatrix}= \\begin{pmatrix} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_p \\end{pmatrix} - \\sum_{j = 1}^{m-1} Z_j \\phi_j\n",
    "\\tag{1.12}\n",
    "\\end{align}\n",
    "\n",
    "and solving the former problem for $\\text{Var}(Z_m)$ using the random variables $\\begin{pmatrix} X_1^{(m)} & X_2^{(m)} & \\dots & X_p^{(m)} \\end{pmatrix}'$ to get $\\phi_m$. It turns out that $\\pmb \\phi$ equals the matrix $\\pmb V = \\begin{pmatrix} v_1 & v_2 & \\dots & v_M \\end{pmatrix} $, whereby $v_i$ is the eigenvector with length one of $\\pmb C_X$ to the corresponding $i$-th highest eigenvalue $\\lambda_i$. Hence, we can compute $\\pmb \\phi$ by only computing the eigenvectors of $\\pmb C_X$ and sort them in decending order by the corresponding eigenvalues. Note that therefore $\\pmb \\phi$ is an orthonormal matrix. Subsequent, $\\pmb Z$ is obtained by matrix multiplication of $\\pmb X$ and $\\pmb \\phi$ as in formula *(1.5)*.\n",
    "\n",
    "**Computed Principal Components have covariance zero**\n",
    "\n",
    "To show that the PC have covariances of zero, I compute the VCV matrix of the $Z_i$'s with respect to the derived $\\pmb \\phi$ matrix. Remember from equation *(1.7)* that the expected value is zero for every random Z variable.\n",
    "\n",
    "\\begin{align}\n",
    "\\pmb C_Z =& \\text{E} \\left( \\begin{pmatrix} Z_1 \\\\ Z_2 \\\\ \\vdots \\\\ Z_p \\end{pmatrix} \\begin{pmatrix} Z_1 & Z_2 & \\dots & Z_p \\end{pmatrix} \\right) = \\text{E} \\pmb \\phi' \\left( \\begin{pmatrix} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_p \\end{pmatrix} \\begin{pmatrix} X_1 & X_2 & \\dots & X_M \\end{pmatrix} \\pmb \\phi \\right) = \\pmb \\phi' \\pmb C_X  \\pmb \\phi = \\\\\n",
    "\\stackrel{\\text{eigendecomposition}}{=}& \n",
    "\\pmb \\phi'\n",
    "\\begin{pmatrix} \n",
    "    v_1 & \\dots & v_M \n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix} \n",
    "    \\lambda_1 & \\dots & 0\\\\\n",
    "    \\vdots & \\vdots & \\vdots \\\\\n",
    "    0 & \\dots & \\lambda_M\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix} \n",
    "    v'_1 \\\\ \\vdots \\\\ v'_M \n",
    "\\end{pmatrix}\n",
    "\\pmb \\phi =\n",
    "\\underbrace{\\pmb \\phi'\n",
    "\\pmb \\phi}_{= I_{p \\times p}}\n",
    "\\begin{pmatrix} \n",
    "    \\lambda_1 & \\dots & 0\\\\\n",
    "    \\vdots & \\vdots & \\vdots \\\\\n",
    "    0 & \\dots & \\lambda_M\n",
    "\\end{pmatrix}\n",
    "\\underbrace{\\pmb \\phi'\n",
    "\\pmb \\phi}_{= I_{p \\times p}} = \\begin{pmatrix} \n",
    "    \\lambda_1 & \\dots & 0\\\\\n",
    "    \\vdots & \\vdots & \\vdots \\\\\n",
    "    0 & \\dots & \\lambda_M\n",
    "\\end{pmatrix}\n",
    "\\tag{1.13}\n",
    "\\end{align} \n",
    "\n",
    "**Portion of cariance captured by the $\\lambda_i$'s**\n",
    "\n",
    "What we also learn from the derivation of equation *(1.8)* is that $\\text{Var}(Z_i) = \\lambda_i \\forall i \\in I$, the $i$-th highest eigenvalue equals the variance of the $i$-th PC. Hence  \n",
    "\n",
    "**Derive the PC in practice**\n",
    "\n",
    "The common problem in practice is that usually the VCV matrix of the $X_i$'s $\\pmb C_X$ is unknown. Hence, in practice it is necessary to estimate this matrix. From equation *(1.1*) it is known that the means of the $X_i$'s can be estimated unbiased and every matrix $\\pmb X$ can trasformed into a matrix that has columns coming from a random variable with zero mean. From equation *(1.2)* it is known that the VCV matrix of the $X_i$'s can be estimated by $\\frac{1}{n-1} \\pmb X' \\pmb X$. We can use this estimate to compute the optimization problem in *(1.9)* with the estimated variances and covariances of the $X_i$'s. However, one can also apply the same fashion as in *(1.2)* to directly compute the estimated VCV matrix for the $Z_m$'s by $\\frac{1}{n-1} \\hat{\\pmb Z}' \\hat{\\pmb Z}$. I use hat notation for the Z matrices, since the variance of $Z_m$ in equation *(1.9)* is estimated, $\\phi_m$ is also estimated and therefore a random variable. Therefore $z_m$ is also estimated, by replacing the true value of $\\phi_m$ in equation *(1.5)* with the estimated value. Consistently, I denote the estimated vectors by $\\hat{\\phi}_m$ and $\\hat{z}_m$. \\\n",
    "In the next step the values in $\\hat{\\pmb \\phi}$ are computed recursively. First, the vector $\\hat{\\phi_1}$ is computed in the same fashion as in *(1.9)* but with estiamted variance. Hence, the empirical maximization problem is\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\phi}_1 = \\arg \\max_{||w|| = 1} \\left( \\frac{1}{n-1} \\hat{z}'_1 \\hat{z}_1 \\right)= \\arg \\max_{||w|| = 1} \\left( \\hat{z}'_1 \\hat{z}_1 \\right)= \\arg \\max_{||w|| = 1} \\left( (\\pmb X w)'(\\pmb X w) \\right) = \\arg \\max_{||w|| = 1} \\left( w' \\pmb X' \\pmb X w\\right)\n",
    "\\tag{1.14}\n",
    "\\end{align}\n",
    "\n",
    "From here onward, the solution is the same as for the theoretical case but the estiamted variables are used. Thus, I have moved a detailed description to the appendix (*A.1*). Since **describe which avriables are estimated and that this is investigated**\n",
    "\n",
    "## 2.3 Principal Component Regression (PCR)\n",
    "be the matrix of indepedent variables and $\\varepsilon \\in \\mathbb{M}_{1 \\times n}$ a random vector.\n",
    "The data generating process is $Y = \\pmb X \\beta + \\varepsilon$. For ease of notation, we assume the expected value of any covariate and of the dependent variable to be $0$. Hence, $\\text{E} ( \\pmb X ) = \\pmb0$ i.e. $\\text{E}(X_i) = 0$ for $\\forall i$. Since we can demean any linear model, this is not restrictive in any sense.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Structural Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Appendix\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A.1 Derivation of the empirical PC**\n",
    "\n",
    "From equation *(1.1*) it is known that the means of the $X_i$'s can be estimated unbiased and every matrix $\\pmb X$ can trasformed into a matrix that has columns coming from a random variable with zero mean. From equation *(1.2)* it is known that the VCV matrix of the $X_i$'s can be estimated by $\\frac{1}{n-1} \\pmb X' \\pmb X$. We can use this estimate to compute the optimization problem in *(1.9)* with the estimated variances and covariances of the $X_i$'s. However, one can also apply the same fashion as in *(1.2)* to directly compute the estimated VCV matrix for the $Z_m$'s by $\\frac{1}{n-1} \\hat{\\pmb Z}' \\hat{\\pmb Z}$. I use hat notation for the Z matrices, since the variance of $Z_m$ in equation *(1.9)* is estimated, $\\phi_m$ is also estimated and therefore a random variable. Therefore $z_m$ is also estimated, by replacing the true value of $\\phi_m$ in equation *(1.5)* with the estimated value. Consistently, I denote the estimated vectors by $\\hat{\\phi}_m$ and $\\hat{z}_m$. \\\n",
    "In the next step the values in $\\hat{\\pmb \\phi}$ are computed recursively. First, the vector $\\hat{\\phi_1}$ is computed in the same fashion as in *(1.9)* but with estiamted variance. Hence, the maximization problem at hand is\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\phi}_1 = \\arg \\max_{||w|| = 1} \\left( \\frac{1}{n-1} \\hat{z}'_1 \\hat{z}_1 \\right)= \\arg \\max_{||w|| = 1} \\left( \\hat{z}'_1 \\hat{z}_1 \\right)= \\arg \\max_{||w|| = 1} \\left( (\\pmb X w)'(\\pmb X w) \\right) = \\arg \\max_{||w|| = 1} \\left( w' \\pmb X' \\pmb X w\\right)\n",
    "\\tag{1.20}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathscr{L}(w,\\lambda) =& w' \\pmb X' \\pmb X w - \\lambda \\left( w'w-1 \\right) \\notag \\\\\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial \\lambda} =& w'w -1 = 0 \\notag \\\\\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial w} =& 2 \\pmb X' \\pmb X w- 2\\lambda w = 0\n",
    "    \\tag{1.21}\n",
    "\\end{align}\n",
    "\n",
    "From equation *(1.21)* it follows that $\\left(\\pmb X' \\pmb X\\right) w = \\lambda w$. Hence, the solution vector w is an eigenvector of $\\pmb X' \\pmb X$ with eigenvalue $\\lambda$. Since $\\pmb X' X$ is a $p \\times p$ matrix, it has $p$ eigenvalues $\\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_p > 0$ with p corresponding unit-length eigenvectors $v_1, v_2, \\dots, v_p$. All $\\lambda_i$ are greather than zero, since $X' X$ is positive definite. Thus, the question arises which eigenvalue to use to maximize the variance. Since we can express any eigenvector as a function of the corresponding eigenvalue, it is sufficient to maximize over the eigenvalue.\n",
    "\n",
    "\\begin{align}\n",
    "    \\arg \\max_{\\lambda \\in \\{\\lambda_1, \\dots, \\lambda_p \\}} w' \\pmb X' \\pmb X w \\stackrel{*(1.21)*}{=}  \\arg \\max_{\\lambda \\in \\{\\lambda_1, \\dots, \\lambda_p \\}} w' \\lambda w = \\arg \\max_{\\lambda \\in \\{\\lambda_1, \\dots, \\lambda_p \\}} \\underbrace{w' w}_{1} \\lambda = \\arg \\max_{\\lambda \\in \\{\\lambda_1, \\dots, \\lambda_p \\}} \\lambda\n",
    "    \\tag{1.22}\n",
    "\\end{align}\n",
    "\n",
    "Hence, the maximum variance is captured by choosing the largest eigenvalue, which is by definition, $\\lambda_1$. The corresponding eigenvector $v_1$ is then chosen to be $\\hat{\\phi_1}$. From equation *(1.4)* it follows that $\\frac{1}{n-1} \\pmb X' \\pmb X$ and $\\pmb X' \\pmb X$ have the same eigenvectors but eigenvalues with the same order multiplied by the scalar. Since it is enough to compute the largest eigenvalue of the estimated VCV matrix of all the $X_i$ to compute $\\hat{\\phi}_1$ and with that $z_1$. Armed with $\\hat{\\phi}_1$, it is possible to compute $\\hat{\\phi}_2, \\hat{\\phi}_3, \\dots, \\hat{\\phi}_M$ iteratively by setting $\\pmb X_m = \\pmb X - \\sum_{j = 1}^{m-1} X \\hat{\\phi}'_j \\hat{\\phi}_j$ and solving the former problem for $\\pmb X_m$ to get $\\hat{\\phi}_m$.\n",
    "\n",
    "It turns out that $\\hat{\\pmb \\phi}$ equals the matrix $\\pmb V = \\begin{pmatrix} v_1 & v_2 & \\dots & v_M \\end{pmatrix} $, whereby $v_i$ is the eigenvector with length one of $X'X$ to the corresponding $i$-th highest eigenvalue $\\lambda_i$. Hence, we can compute $\\hat{\\pmb \\phi}$ by only computing the eigenvectors of $X'X$ and sort them in decending order by the corresponding eigenvalues. Subsequent, $\\hat{\\pmb Z}$ is obtained by matrix multiplication of $\\pmb X$ and $\\hat{\\pmb \\phi}$ as in formula *(1.5)*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivaton of pca\n",
    "https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch18.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get number of PCA components\n",
    " Kaiser  criterion(Guttman, 1954; Kaiser, 1960)\n",
    " acceleration  factor(Cattell,  1966; Raiche,  Roipel,  and  Blais,2006)and parallel analysis(Horn, 1965)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comparison of pca and ridge\n",
    "https://www.researchgate.net/publication/259265422_A_Monte_Carlo_Comparison_between_Ridge_and_Principal_Components_Regression_Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction\n",
    "- Black  (1997)  shows  that  parents  arewilling to pay a premium to buy a house in a neighborhood with aschool that scores well. (Do  Better  Schools  Matter?  Parental  Valuation  ofElementary  Education) - Havard paper\n",
    "- https://www.researchgate.net/profile/Duncan_Thomas/publication/5194918_Early_Test_Scores_Socioeconomic_Status_and_Future_Outcomes/links/575812c508ae5c6549074510/Early-Test-Scores-Socioeconomic-Status-and-Future-Outcomes.pdf nice to find literature on test scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. structural model with equation\n",
    "2. relationship $y = X \\beta + \\epsilon$\n",
    "    Xs are correlated -> pcr\n",
    "3. simulate structural model using parameter\n",
    "4. see how large the variance is of the pcr beta and compare to the ones estimated. (can be computed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
